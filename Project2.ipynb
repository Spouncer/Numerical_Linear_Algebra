{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2\n",
    "\n",
    "This assessment is marked out of 100 and comprises 20% of the final course mark.\n",
    "\n",
    "Due by 4pm on Thursday December 2nd, to be submitted on Learn.\n",
    "\n",
    "### Academic misconduct\n",
    "\n",
    "The assessment is summative in nature. You are expected to be aware of and abide by University policies on academic misconduct.\n",
    "\n",
    "- [School of Mathematics academic misconduct advice and policies](https://teaching.maths.ed.ac.uk/main/undergraduate/studies/assessment/academic-misconduct)\n",
    "- [Academic Services academic misconduct information](https://www.ed.ac.uk/academic-services/students/conduct/academic-misconduct)\n",
    "\n",
    "**This is an individual assignment - do not copy the work of another student or show your own work to another student.**\n",
    "\n",
    "If you use any resources (e.g. textbooks or websites), then you must include appropriate references in your solutions. Course materials do not need to be referenced, but you should clearly state which results you are using.\n",
    "\n",
    "\n",
    "### Code commentary\n",
    "\n",
    "Your code should be extensively commented, with the functionality of each line of code explained with a comment. This is to test your understanding of the code you have written. Up to half of the marks associated with the coding part of a question may be deducted for a missing, incomplete, or inaccurate code commentary.\n",
    "\n",
    "The following provides an example of the expected level of commenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def LU(A):\n",
    "    \n",
    "    # Find dimension of A\n",
    "    n = A.shape[0]\n",
    "    \n",
    "    # Initialise L=I, U=A\n",
    "    L = np.eye(n) \n",
    "    U = np.copy(A)\n",
    "\n",
    "    for k in range(n - 1): # loop over columns 1 to n-1\n",
    "        for j in range(k + 1, n): # loop over rows k+1 to n\n",
    "            L[j, k] = U[j, k] / U[k ,k] # compute the multiplier l_jk\n",
    "            U[j, k:] = U[j, k:] - L[j, k] * U[k, k:] # subtract a multiple of row k from row j to create zeros \n",
    "                                                     # below the diagonal in column k\n",
    "    \n",
    "    return L, U # return the LU factorisation of A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code efficiency\n",
    "\n",
    "To obtain full marks, your code should be *efficient* in the sense of avoiding unnecessary artihmetic operations and having low computational cost. \n",
    "\n",
    "### Output \n",
    "\n",
    "Your code must generate and display all relevant output when run. Rerun your code cells after editing your code, to make sure that the output is updated and displayed when you submit your notebook.\n",
    "\n",
    "### Re-using code and built-in functions\n",
    "\n",
    "You can re-use your own code from previous workshops. You can also use the model solutions for workshop exercises posted on Learn, and Jupyter notebooks from lecture material posted on Learn. You do NOT need to comment re-used code, but you should clearly indicate from where you have taken the code. \n",
    "\n",
    "You may use any built-in Python functions as required.\n",
    "\n",
    "### Written exercises\n",
    "\n",
    "You can enter your answers to theoretical questions, i.e. the discussion in question 1.2, questions 2.1, 2.2, 2.5, 3.2 and 3.3, in the Markdown cells provided in this notebook. To start editing the cell, press shift+enter or double click on it. You can use basic Latex. To render the cell, press shift+enter or run.\n",
    "\n",
    "Alternatively, you can submit your hand-written and scanned answers to the theoretical questions as a pdf on Learn, alongside this notebook. \n",
    "\n",
    "To obtain full marks, you should provide a clear and justified argument written in full sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 \n",
    "\n",
    "Let $\\bf A \\in \\mathbb R^{m \\times n}$, with $m \\geq n$ and $rank({\\bf A}) = n$, and ${\\bf b} \\in \\mathbb R^m$. Consider the following algorithm to solve the least squares problem of finding $\\bf x \\in \\mathbb R^n$ that minimises $\\| \\bf A \\bf x - \\bf b\\|_2$.\n",
    "\n",
    "**Algorithm LSQ-PI** \n",
    "\n",
    "*Input: $\\bf A \\in \\mathbb R^{m \\times n}$, with $m \\geq n$ and $rank({\\bf A}) = n$, and $\\bf b \\in \\mathbb R^m$*\n",
    "\n",
    "*Output: $\\bf x \\in \\mathbb R^n$ that minimises $\\| \\bf A \\bf x - \\bf b\\|_2$*\n",
    "\n",
    "1. Compute the pseudo-inverse $\\bf A^\\dagger$\n",
    "2. Compute $\\bf x = A^\\dagger b$ using Algorithm MV (matrix-vector multiplication)\n",
    "\n",
    "In lectures, we defined the pseudo-inverse $ {\\bf A^\\dagger = (\\bf A^\\mathrm T \\bf A)^{-1} A^\\mathrm T}$. \n",
    "\n",
    "Let $\\mathbf{A} = \\mathbf{U \\Sigma V}^\\mathrm{T}$ be a singular value decomposition. In Workshop week 10, we will see that $ \\mathbf{A}^\\dagger = \\mathbf{V \\Sigma^\\dagger U}^\\mathrm{T}$, where ${\\bf \\Sigma^\\dagger} \\in \\mathbb R^{n \\times m}$ is obtained by taking the reciprocal of the diagonal elements of $\\bf \\Sigma$, and then transposing the matrix. Under the assumptions in this question, this is equivalent to $ \\mathbf{A}^\\dagger = \\mathbf{V \\Sigma_1^{-1} U_1}^\\mathrm{T}$, where $\\mathbf{A} = \\mathbf{U_1 \\Sigma_1 V}^\\mathrm{T}$ is a reduced singular value decomposition.\n",
    "\n",
    "So we can consider two different versions of Algorithm LSQ-PI:\n",
    "\n",
    "1. Computing the pseudo-inverse in step 1 as $ {\\bf A^\\dagger = (\\bf A^\\mathrm T \\bf A)^{-1} A^\\mathrm T}$, i.e. by finding $(\\bf A^\\mathrm T \\bf A)^{-1}$ and multiplying it by $\\mathbf{A}^\\mathrm{T}$.\n",
    "\n",
    "2. Computing the pseudo-inverse in step 1 as $ \\mathbf{A}^\\dagger = \\mathbf{V \\Sigma_1^{-1} U_1}^\\mathrm{T}$.\n",
    "\n",
    "### Question 1.1 \n",
    "\n",
    "Implement the two versions of Algorithm LSQ-PI in the code cell below. The inputs to your functions should be $\\mathbf{A}$ and $\\mathbf{b}$, and the output should be the minimiser $\\mathbf{x}$.Test your code on the example given at the bottom of the code cell, to which the solution is $x = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$.\n",
    "\n",
    "**[12 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def LSQ_PI_1(A, b):\n",
    "    ''' Uses A transpose A inverse and A transpose\n",
    "    to find the pseudo inverse of A to return the minimum x value of ||Ax - b \n",
    "    -----------------\n",
    "    Inputs: A = mxn matrix m>=n\n",
    "    b = m dimensional vector\n",
    "    Outputs: x = n dimensional vector that minimises ||Ax - b||2.\n",
    "    '''\n",
    "    AT = A.T # Find transpose of A\n",
    "    pA_inv = np.linalg.solve(AT@A, AT) # Calculate pseudo inverse of A\n",
    "    x = pA_inv@b # Find x\n",
    "    return x\n",
    "\n",
    "def LSQ_PI_2(A, b):\n",
    "    ''' Uses the SVD of A to find the pseudo inverse of A\n",
    "        to return the minimum x value of ||Ax - b||2.\n",
    "        -----------------\n",
    "        Inputs: A = mxn matrix m>=n\n",
    "        b = m dimensional vector\n",
    "    Outputs: x = n dimensional vector that minimises ||Ax - b||2.\n",
    "    '''\n",
    "    U, Sig, VT = np.linalg.svd(A, full_matrices = False) # Find reduced SVD d\n",
    "    Sig_inv = np.ones(len(Sig))/Sig # Find the diagonal values of sigma inver \n",
    "    pA_inv = VT.T@np.diag(Sig_inv)@U.T # Matrix multiply V with sigma inverse\n",
    "    x = pA_inv@b # Find x \n",
    "    return x\n",
    "\n",
    "# Test your code on a small example\n",
    "b = np.array([0., 1., 2.])\n",
    "A = np.array([[1., -0.5],[1., 0.],[1., 0.5]]) \n",
    "x1 = LSQ_PI_1(A, b)\n",
    "x2 = LSQ_PI_2(A, b)\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2\n",
    "\n",
    "Investigate the performance of the two versions of Algorithm LSQ-PI in terms of the error in the computed solution as a function of $n$ (where $\\mathbf{A} \\in \\mathbb{R}^{2n \\times n}$) and $\\kappa_2(\\mathbf{A})$.\n",
    "\n",
    "Include any tests that you run in the code cell below and display your results. Do you notice any trends in the results? How do you explain the difference in accuracy of the two versions of the algorithm? \n",
    "\n",
    "**[15 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def randsvd(m, n, kappa):\n",
    "    '''Taken from week 7 jupyter notebook for lectures. Generates a random mxn matrix with kappa value kappa.''' s = np.zeros(n)\n",
    "    S = np.zeros((m,n))\n",
    "    for i in range(n):\n",
    "        beta = kappa**(1/(n-1))\n",
    "        s[i] = beta**(-i) S[:n,:n] = np.diag(s)\n",
    "    def haar(n):\n",
    "        A = np.random.randn(n, n) Q, R = np.linalg.qr(A)\n",
    "        for i in range(n): if R[i, i] < 0:\n",
    "        Q[:, i] *= -1 \n",
    "        return Q\n",
    "    A = haar(m) @ S @ haar(n).T \n",
    "    return A\n",
    "\n",
    "# Define number of kappa and n values to test\n",
    "a = 20 \n",
    "c=9\n",
    "# Initialise arrays of kappa, kappa squared, 2-norm of x and n values\n",
    "kappa = np.zeros(a)\n",
    "norm_xsol = np.zeros(c)\n",
    "n_vals = np.array(np.zeros(c), dtype = int)\n",
    "for k in range(a): #Generate range of kappa values \n",
    "    kappa[k] = (4*k + 2)**5\n",
    "\n",
    "for i in range(c): #Generate range of n values \n",
    "    n_vals[i] = int((i + 4)**3)\n",
    "\n",
    "for n in range(c): # Caluclate 2-norms of x for each n \n",
    "    xsol = np.ones(n_vals[n])\n",
    "norm_xsol[n] = np.linalg.norm(xsol, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Initialise matrices of errors for each method \n",
    "errors1 = np.zeros((a, c))\n",
    "errors2 = np.zeros((a, c))\n",
    "for k in range(a): # Loop over kappa values\n",
    "    for i in range(c): #Loop over 3 2nxn matrices\n",
    "        n = n_vals[i]\n",
    "# Call n values and generate random matrix\n",
    "A = randsvd(2*n, n, kappa[k])\n",
    "# Define solution\n",
    "xsol = np.ones(n)\n",
    "# Calculate input b b = A@xsol\n",
    "# Find approximate solution\n",
    "x1 = LSQ_PI_1(A, b) \n",
    "x2 = LSQ_PI_2(A, b)\n",
    "# Save normalised errors in matrices\n",
    "errors1[k][i] = np.linalg.norm(xsol-x1,2)/norm_xsol[i] \n",
    "errors2[k][i] = np.linalg.norm(xsol-x2,2)/norm_xsol[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up figure for plots of errors\n",
    "fig, ax = plt.subplots(3, figsize = (14, 14))\n",
    "fig2, ax2 = plt.subplots(3, figsize = (14, 14)) \n",
    "fig.suptitle('Error comparison LSQ SVD and pseudo inverse')\n",
    "\n",
    "for i in range(3): # Loop over each n, plotting errors against condition numb \n",
    "    ax[i].plot(kappa, errors1[:,3*i ], label = f'n = {n_vals[3*i]}, LSQ_PI1') \n",
    "    ax[i].plot(kappa, errors2[:,3*i ], label = f'n = {n_vals[3*i]}, LSQ_PI2') \n",
    "    ax[i].set_xscale('log')\n",
    "    ax[i].set_yscale('log') \n",
    "    ax[i].set_xlabel('log(kappa)') \n",
    "    ax[i].set_ylabel('log(Error)')\n",
    "\n",
    "    ax2[i].plot(n_vals, errors1[6*i,:], label = f'k = {kappa[i]}, LSQ_PI1') \n",
    "    ax2[i].plot(n_vals, errors2[6*i,:], label = f'k = {kappa[i]}, LSQ_PI2') \n",
    "    ax2[i].set_xscale('log')\n",
    "    ax2[i].set_yscale('log')\n",
    "    ax2[i].set_xlabel('log(n)') \n",
    "    ax2[i].set_ylabel('log(Error)')\n",
    "    ax[i].legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Q1.2**\n",
    "\n",
    "*Add discussion here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "This question concerns polynomial regression. Suppose we have a set of $m$ observation points $\\{a_i, b_i\\}_{i=1, \\dots, m}$. As in the lecture notes, we can fit a polynomial of degree $n-1$ to the data by choosing the coefficients $\\mathbf{x}^\\mathrm{LS} \\in \\mathbb R^n$ to minimise $\\|\\mathbf{A x} - \\mathbf{b}\\|_2^2 $, where $\\mathbf{A} \\in \\mathbb{R}^{m\\times n}$ is the Vandermonde matrix with entries $a_{ij} = (a_i)^{j-1}, i=1, \\dots, m, j=1, \\dots, n$,  and $\\mathbf{b} = [b_1, b_2, \\dots, b_m]^T$.\n",
    "\n",
    "As we saw in Exercise 1 in Computer lab week 9, the polynomials fitted in this way often lead to *overfitting*: \n",
    "\n",
    "A more complex model is not necessarily a better model. Overfitting occurs when a model is complex enough to fit very closely to a particular set of data points, but does not generalise well, and will not be suitable to predict future observations. Instead of capturing the general relationship between input and output in a dataset, an overfitted model describes the noise in the observations.\n",
    "\n",
    "A method to avoid overfitting is *regularisation*. Instead of minimising $\\|\\mathbf{A x} - \\mathbf{b}\\|_2^2 $, we seek to minimise the new cost function\n",
    "\n",
    "$$\n",
    "\\|\\mathbf{A x} - \\mathbf{b}\\|_2^2  + \\mu \\|\\mathbf{x} \\|_2^2,\n",
    "$$\n",
    "\n",
    "where $\\mu>0$ is a regularisation parameter. The idea is to introduce a penalty term, so that the polynomial coefficients $\\mathbf{x}$ (particularly the high-order coefficients) remain relatively small. \n",
    "\n",
    "Denote by $\\mathbf{x}^\\mathrm{regLS}$ the minimiser of $\\|\\mathbf{A x} - \\mathbf{b}\\|_2^2  + \\mu \\|\\mathbf{x} \\|_2^2$.\n",
    "\n",
    "### Question 2.1\n",
    "\n",
    "Show that $\\mathbf{x}^\\mathrm{regLS}$ solves the regularised normal equations\n",
    "\n",
    "$$\n",
    "(\\mathbf{A}^{\\mathsf{T}} \\mathbf{A} + \\mu \\mathbf{I}) \\mathbf{x}^\\mathrm{regLS} = \\mathbf{A}^{\\mathsf{T}} \\mathbf{b}.\n",
    "$$\n",
    "\n",
    "**[6 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Q2.1**\n",
    "\n",
    "*Double click on this cell to start typing and editing. Run the cell when you are done typing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Question 2.2\n",
    "\n",
    "Let $\\mathbf{A} = \\mathbf{U_1 \\Sigma_1 V}^{\\mathsf{T}}$ be the reduced SVD of $\\mathbf{A}$. Using Question 2.1, or otherwise, show that $\\mathbf{x}^\\mathrm{regLS}$ satisfies\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^\\mathrm{regLS} = \\mathbf{VS} \\mathbf{U_1}^{\\mathsf{T}} \\mathbf{b},\n",
    "$$\n",
    "\n",
    "where the matrix $\\mathbf{S}\\in \\mathbb{R}^{n\\times n}$ is diagonal with diagonal entries given by\n",
    "\n",
    "$$\n",
    "s_{ii} = \\frac{\\sigma_i}{\\sigma_i^2 + \\mu}, \\qquad i=1,\\ldots,n,\n",
    "$$\n",
    "with $\\sigma_i$ the singular values of $\\mathbf{A}$.\n",
    "\n",
    "**[10 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Q2.2**\n",
    "\n",
    "*Double click on this cell to start typing and editing. Run the cell when you are done typing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3\n",
    "\n",
    "Implement a Python function called LSQ_SVD_reg, which computes $\\mathbf{x}^\\mathrm{regLS}$ using the expression found in Question 2.2. The inputs to your function should be $\\mathbf{A}$, $\\mathbf{b}$ and $\\mu$, and the output should be the minimiser $\\mathbf{x}^\\mathrm{regLS}$. Test your code on the example given at the bottom of the code cell, to which the solution is $x = \\begin{bmatrix} 0.6 \\\\ 0.4 \\end{bmatrix}$.\n",
    "\n",
    "**[8 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def LSQ_SVD_reg(A, b, mu):\n",
    "    '''Returns a regularised least squares value of x. \n",
    "    -------------------------------\n",
    "    Inputs: A = mxn vandermonde matrix\n",
    "    b = array of observed values for each data point\n",
    "    mu = regularisation constant.\n",
    "    Outputs: x = least squares estimate, array of coefficients of \n",
    "    n-1 degree polynomial, approximating the data.'''\n",
    "    # Reduced singular value decomposition\n",
    "    U, Sig, VT = np.linalg.svd(A, full_matrices = False)\n",
    "        # Redefine singular values using regularisation constant\n",
    "    Sig_mu = (mu*np.ones(len(Sig)) + Sig*Sig) \n",
    "    S = np.diag(Sig/Sig_mu)\n",
    "        # Use expression derives in Q2.2 to find x\n",
    "    x = VT.T@S@U.T@b \n",
    "    return x\n",
    "# Test your code on a small example\n",
    "b = np.array([0., 1., 2.])\n",
    "A = np.array([[1., -0.5],[1., 0.],[1., 0.5]]) \n",
    "mu = 2.0\n",
    "x = LSQ_SVD_reg(A, b,mu)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4\n",
    "\n",
    "We now want to test the effect of regularisation on a particular observed data set. Implement the following in Python in the code cell below, using the code from Exercise 1 in Computer lab week 9 (or otherwise):\n",
    "\n",
    "(i) Download the file *data_points_30.npy* from Learn, and load it into an array using np.load(). This is a 30x2 array containing a set of 30 data points $\\{a_i, b_i\\}_{i=1, \\dots, 30}$ with the $a_i$ in the first column and the $b_i$ in the second column. Plot the data points.\n",
    "\n",
    "(ii) Set up the Vandermonde matrix $\\mathbf{A}$ of size $30 \\times n$ for $n=2,4,6,8,10$, and compute the (un-regularised) polynomial coefficients $\\mathbf{x}^\\mathrm{LS}$ for the polynomial of degree $n-1$. On the same graph as the data points, plot your fitted polynomials over the interval $[0, 6]$.\n",
    "\n",
    "(ii) Use your function *LSQ_SVD_reg* from Question 2.3 to compute the the (regularised) polynomial coefficients $\\mathbf{x}^\\mathrm{LSreg}$ for the polynomial of degree $n-1$ for $n=2,4,6,8,10$, for $\\mu=5, 0.5, 0.01,$ and $10^{-5}$. For each value of $\\mu$, produce the same plot as in (i)-(ii).\n",
    "\n",
    "You should end up with 5 plots showing the data points in *data_points_30.npy* and the fitted polynomials for $n=2,4,6,8,10$: 1 for the standard minimiser $\\mathbf{x}^\\mathrm{LS}$, and 4 for the regularised minimiser $\\mathbf{x}^\\mathrm{LSreg}$ (1 for each of the 4 values of $\\mu$).\t\n",
    "\n",
    "**[8 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "M = np.load('data_points_30.npy copy') a = M[:, 0]\n",
    "b = M[:, 1]\n",
    "m = M.shape[0]\n",
    "\n",
    "# Initialise plot\n",
    "fig, ax = plt.subplots(5, figsize=(9, 20))\n",
    "fig.suptitle('LSQ n-1 degree polynomial approximations', fontsize=16) \n",
    "x_plot = np.linspace(0, 6, 200)\n",
    "mu_vals = [0, 10**(-5), 0.01 ,0.5, 5 ]\n",
    "# mu = 0 is equivalent to LSQ_SVD (unregularised)\n",
    "for r in range(len(mu_vals)): # Loop over mu value, generating plot for each \n",
    "    ax[r].plot(a, b, 'kx')\n",
    "    print(f'\\n Polynomial coefficients for mu = {mu_vals[r]}')\n",
    "for n in range(1, 6):# Plot all n-1 degree polynomials for each mu\n",
    "    # Form Vandermonde matrix\n",
    "    A = np.array([[i**k for k in range(2*n)] for i in a], dtype=float)\n",
    "            # Compute the minimiser x\n",
    "    x = LSQ_SVD_reg(A, b, mu_vals[r])\n",
    "    print(f' n = {2*n}: {x}')\n",
    "    # Plot the polynomials\n",
    "    p_plot = np.polyval(x[::-1], x_plot) \n",
    "    ax[r].plot(x_plot, p_plot, label='n = {}'.format(2*n)) \n",
    "    ax[r].set_xlim([0, 6])\n",
    "    ax[r].set_ylim([0, 6]) \n",
    "    ax[r].legend()\n",
    "            # Generate appropriate plot titles\n",
    "    if r != 0:\n",
    "        ax[r].set_title(f'Regularised least squares approximations, mu = {mu_vals[r]}')\n",
    "    else:\n",
    "        ax[r].set_title('Unregularised least squares aproximations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.5\n",
    "\n",
    "Discuss your plots in Question 2.4. How does regularisation affect the fitted polynomials in terms of their fit to the data and their usefulness for extrapolation? How does regularisation affect the coefficient values? What happens when $\\mu$ is too large or too small, and can you explain why this happens? Which value of $\\mu$ produces the best results in your opinion? \n",
    "\n",
    "**[15 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Q2.5**\n",
    "\n",
    "*Double click on this cell to start typing and editing. Run the cell when you are done typing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "Let $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ be symmetric. Consider the following extension of Algorithm SII (Shifted Inverse Iteration), known as Rayleigh Quotient Iteration.\n",
    "\n",
    "**Algorithm RQI:**\n",
    "\n",
    "*Input*: $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$ symmetric, $\\mathbf z^{(0)} \\in \\mathbb R^n \\setminus \\{\\mathbf 0 \\}$, $\\varepsilon > 0$\n",
    "\n",
    "*Output*: $\\mathbf z^{(k)} \\in \\mathbb R^n$, $\\lambda^{(k)} \\in \\mathbb R$, with $\\mathbf z^{(k)} \\approx \\mathbf  x_j$ and $\\lambda^{(k)} \\approx \\lambda_j$ for some $j \\in \\{1, \\dots, n\\}$. \n",
    "\n",
    "1. $k=0$ \n",
    "\n",
    "2. $\\lambda^{(0)} = r_\\mathbf{A}(\\mathbf{z}^{(0)})$\n",
    "\n",
    "3. While $\\|\\mathbf A \\mathbf z^{(k)} - \\lambda^{(k)} \\mathbf z^{(k)}\\|_2 > \\varepsilon$\n",
    "\n",
    "4. $\\quad$ $k = k+1$\n",
    "\n",
    "5. $\\quad$ Solve $(\\mathbf{A} - \\lambda^{(k-1)} \\mathbf{I}) \\, \\mathbf z^{(k)} = \\mathbf z^{(k-1)}$\n",
    "\n",
    "6. $\\quad$ $\\mathbf z^{(k)} = \\frac{\\mathbf z^{(k)}}{\\|\\mathbf z^{(k)}\\|_2}$\n",
    "\n",
    "7. $\\quad$ $\\lambda^{(k)} = r_\\mathbf{A}(\\mathbf{z}^{(k)})$\n",
    "\n",
    "8. End While\n",
    "\n",
    "The main idea of Rayleigh quotient iteration is to replace the shift $s$ in ALgorithm SII with the estimate $\\lambda^{(k-1)}$ of the eigenvalue to speed up convergence. If Rayleigh quotient iteration converges, it converges to an eigenvector and eigenvalue of $\\mathbf{A}$, but which eigenvector/-value it converges to will depend on the starting guess $\\mathbf z^{(0)}$.\n",
    "\n",
    "In line 5 of Algorithm RQI, suppose we want to solve the system using Algorithm GEPP (Gaussian elimination with partial pivoting).\n",
    "\n",
    "### Question 3.1\n",
    "\n",
    "In the code cell below, write a function *RQI* that implements Algorithm RQI following the pseudo-code above.\n",
    "\n",
    "Test your code on the example given at the bottom of the code cell, for which the outputs should converge to  $\\mathbf x_2 = - \\frac{1}{\\sqrt{10}} \\begin{bmatrix} 3 \\\\ 1 \\end{bmatrix} \\approx \\begin{bmatrix} -0.9486833 \\\\ -0.31622776 \\end{bmatrix}$ and $\\lambda_2 = 1$.\n",
    "\n",
    "**[10 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def RQI(A,z0,eps):\n",
    "    '''Uses Rayleigh Quotient Iteration to find an eigenvalue \n",
    "    and eigenvector that the intial guess z0 is closest to within \n",
    "    a tolerance of eps.\n",
    "    '''\n",
    "    def lamk(zk):\n",
    "        '''Function which finds the rayleigh quotient of zk'''\n",
    "        return zk.T@A@zk/(zk.T@zk)\n",
    "        # Initilise values for iteration\n",
    "    zk = z0\n",
    "    lk = lamk(z0)\n",
    "    norm = np.linalg.norm(A@zk - lk*zk, 2)\n",
    "    while norm > eps: # Loop until tolerance in convergence is acheived\n",
    "        # compute next iteration for the eigenvector\n",
    "        zk_plus = np.linalg.solve(A - lk*np.eye(np.shape(A)[0]), zk)# Normalise eigenvector guess\n",
    "        zk = zk_plus/np.linalg.norm(zk_plus, 2)\n",
    "        # Compute next iteration for the eigenvalue\n",
    "        lk = lamk(zk)\n",
    "        # Check convergence to compare against eps in next iteration norm = np.linalg.norm(A@zk - lk*zk, 2)\n",
    "    return lk, zk\n",
    "\n",
    "\n",
    "# Small test example\n",
    "A = np.array([[2, -3], [-3,10]], dtype=float) \n",
    "z0 = np.array([1,1], dtype=float)\n",
    "eps = 10**(-3)\n",
    "lk, zk = RQI(A,z0,eps) \n",
    "print(lk)\n",
    "print(zk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3.2\n",
    "\n",
    "For the particular $\\mathbf{A}$ given in the example in Question 3.1, derive an expression for the condition number $\\kappa_2(\\mathbf{A} - \\lambda^{(k)} \\mathbf{I})$ in terms of $\\lambda^{(k)}$ and the eigenvalues of $\\mathbf{A}$.\n",
    "\n",
    "*You may use without proof that the eigenvalues of $\\mathbf{A}$ are $\\lambda_1=11$ and $\\lambda_2=1$.*\n",
    "\n",
    "**[6 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Q3.2**\n",
    "\n",
    "*Double click on this cell to start typing and editing. Run the cell when you are done typing.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3.3\n",
    "\n",
    "For the particular $\\mathbf{A}$ given in the example in Question 3.1, what happens to the condition number $\\kappa_2(\\mathbf{A} - \\lambda^{(k)} \\mathbf{I})$ as $k \\rightarrow \\infty$?\n",
    "\n",
    "What implications does the behaviour of $\\kappa_2(\\mathbf{A} - \\lambda^{(k)} \\mathbf{I})$ as $k \\rightarrow \\infty$ have when Algorithm RQI is implemented in floating point arithmetic?\n",
    "\n",
    "Do you think you would see the same behaviour of $\\kappa_2(\\mathbf{A} - \\lambda^{(k)} \\mathbf{I})$ as $k \\rightarrow \\infty$ for a general symmetric matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$? Explain why or why not.\n",
    "\n",
    "**[10 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Q3.3**\n",
    "\n",
    "*Double click on this cell to start typing and editing. Run the cell when you are done typing.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
